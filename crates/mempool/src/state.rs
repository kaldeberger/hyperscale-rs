//! Mempool state.

use hyperscale_core::{Action, Event, OutboundMessage, SubStateMachine, TransactionStatus};
use hyperscale_types::{
    AbortReason, Block, BlockHeight, DeferReason, Hash, NodeId, RoutableTransaction, Topology,
    TransactionAbort, TransactionDecision,
};
use std::collections::{BTreeMap, HashMap, HashSet};
use std::sync::Arc;
use std::time::Duration;
use tracing::instrument;

/// Default RPC mempool limit: 4x block size.
///
/// This provides enough buffer for a few blocks worth of transactions while
/// keeping the pool small enough for efficient state lock checking.
pub const DEFAULT_RPC_MEMPOOL_LIMIT: usize = 4096 * 4; // 16,384 transactions

/// Mempool configuration.
#[derive(Debug, Clone)]
pub struct MempoolConfig {
    /// Maximum number of transactions in the pool before rejecting RPC submissions.
    ///
    /// When the pool reaches this size, new transactions submitted via RPC will be
    /// rejected with a "mempool full" response. This provides backpressure to clients.
    ///
    /// Note: Transactions received via gossip are still accepted even when over this
    /// limit, because other nodes may propose blocks containing those transactions.
    /// We need them in our mempool to validate and vote on those blocks.
    ///
    /// Set to `None` for unlimited (not recommended for production).
    pub max_rpc_pool_size: Option<usize>,
}

impl Default for MempoolConfig {
    fn default() -> Self {
        Self {
            max_rpc_pool_size: Some(DEFAULT_RPC_MEMPOOL_LIMIT),
        }
    }
}

impl MempoolConfig {
    /// Create a new config with default values.
    pub fn new() -> Self {
        Self::default()
    }

    /// Set the maximum RPC pool size.
    pub fn with_max_rpc_pool_size(mut self, max: Option<usize>) -> Self {
        self.max_rpc_pool_size = max;
        self
    }

    /// Create a config with no RPC pool size limit (for testing).
    pub fn unlimited() -> Self {
        Self {
            max_rpc_pool_size: None,
        }
    }
}

/// Lock contention statistics from the mempool.
#[derive(Clone, Copy, Debug, Default)]
pub struct LockContentionStats {
    /// Number of nodes currently locked by in-flight transactions.
    pub locked_nodes: u64,
    /// Number of transactions blocked waiting for a winner to complete.
    pub blocked_count: u64,
    /// Number of transactions in Pending status.
    pub pending_count: u64,
    /// Number of pending transactions that conflict with locked nodes.
    pub pending_blocked: u64,
    /// Number of transactions in Committed status (block committed, being executed).
    pub committed_count: u64,
    /// Number of transactions in Executed status (execution done, awaiting certificate).
    pub executed_count: u64,
}

impl LockContentionStats {
    /// Contention ratio: what fraction of pending transactions are blocked.
    pub fn contention_ratio(&self) -> f64 {
        if self.pending_count > 0 {
            self.pending_blocked as f64 / self.pending_count as f64
        } else {
            0.0
        }
    }
}

/// Entry in the transaction pool.
#[derive(Debug)]
struct PoolEntry {
    tx: Arc<RoutableTransaction>,
    status: TransactionStatus,
    added_at: Duration,
    /// Whether this is a cross-shard transaction (cached at insertion time).
    cross_shard: bool,
}

/// Mempool state machine.
///
/// Handles transaction lifecycle from submission to completion.
/// Uses `BTreeMap` for the pool to maintain hash ordering, which allows
/// ready_transactions() to iterate in sorted order without sorting.
pub struct MempoolState {
    /// Transaction pool sorted by hash (BTreeMap for ordered iteration).
    pool: BTreeMap<Hash, PoolEntry>,

    /// Blocked transactions waiting for their winner to complete.
    /// Maps: loser_tx_hash -> (loser_tx, winner_tx_hash)
    ///
    /// When a deferral commits, the loser is added here with status Blocked.
    /// When the winner's certificate commits, we create a retry.
    blocked_by: HashMap<Hash, (Arc<RoutableTransaction>, Hash)>,

    /// Reverse index: winner_tx_hash -> Vec<loser_tx_hash>
    /// Allows O(1) lookup of all losers blocked by a winner.
    blocked_losers_by_winner: HashMap<Hash, Vec<Hash>>,

    /// Pending deferrals for transactions not yet in the pool.
    /// This handles sync scenarios where a deferral references a transaction
    /// from an earlier block that hasn't been added to the pool yet.
    /// Maps: loser_tx_hash -> (winner_tx_hash, block_height)
    pending_deferrals: HashMap<Hash, (Hash, BlockHeight)>,

    /// Pending retries for transactions not yet in the pool.
    /// This handles sync scenarios where the winner's certificate arrives
    /// before the deferred loser transaction. When the loser arrives,
    /// we immediately create the retry.
    /// Maps: loser_tx_hash -> (winner_tx_hash, cert_height)
    pending_retries: HashMap<Hash, (Hash, BlockHeight)>,

    /// Completed winners whose certificates have been committed.
    /// Used to handle the race condition where a deferral arrives after
    /// the winner has already completed. When a deferral references a
    /// winner in this set, we immediately create the retry.
    /// Maps: winner_tx_hash -> cert_height
    completed_winners: HashMap<Hash, BlockHeight>,

    /// Tombstones for transactions that have reached terminal states.
    /// Prevents re-adding completed/aborted/retried transactions via gossip.
    /// Maps: tx_hash -> block_height when tombstoned (for cleanup)
    tombstones: HashMap<Hash, BlockHeight>,

    /// Cached set of locked nodes (incrementally maintained).
    /// A node is locked if any transaction that declares it is in Committed or Executed status.
    /// This avoids O(n) scan on every ready_transactions() call.
    /// Note: Only one transaction can lock a node at a time (enforced by ready_transactions filtering).
    locked_nodes_cache: HashSet<NodeId>,

    /// Current time.
    now: Duration,

    /// Network topology for shard-aware transaction routing.
    topology: Arc<dyn Topology>,

    /// Current committed block height (for retry transaction creation).
    current_height: BlockHeight,

    /// Configuration for mempool behavior.
    config: MempoolConfig,
}

impl MempoolState {
    /// Create a new mempool state machine with default config.
    pub fn new(topology: Arc<dyn Topology>) -> Self {
        Self::with_config(topology, MempoolConfig::default())
    }

    /// Create a new mempool state machine with custom config.
    pub fn with_config(topology: Arc<dyn Topology>, config: MempoolConfig) -> Self {
        Self {
            pool: BTreeMap::new(),
            blocked_by: HashMap::new(),
            blocked_losers_by_winner: HashMap::new(),
            pending_deferrals: HashMap::new(),
            pending_retries: HashMap::new(),
            completed_winners: HashMap::new(),
            tombstones: HashMap::new(),
            locked_nodes_cache: HashSet::new(),
            now: Duration::ZERO,
            topology,
            current_height: BlockHeight(0),
            config,
        }
    }

    /// Handle transaction submission from client.
    #[instrument(skip(self, tx), fields(tx_hash = ?tx.hash()))]
    pub fn on_submit_transaction_arc(&mut self, tx: Arc<RoutableTransaction>) -> Vec<Action> {
        let hash = tx.hash();

        // Check for duplicate
        if let Some(entry) = self.pool.get(&hash) {
            return vec![Action::EmitTransactionStatus {
                tx_hash: hash,
                status: TransactionStatus::Pending, // Already exists
                added_at: entry.added_at,
                cross_shard: entry.cross_shard,
            }];
        }

        // Reject if tombstoned (already completed/aborted/retried)
        if self.is_tombstoned(&hash) {
            tracing::debug!(tx_hash = ?hash, "Rejecting tombstoned transaction submission");
            return vec![];
        }

        let cross_shard = tx.is_cross_shard(self.topology.num_shards());
        self.pool.insert(
            hash,
            PoolEntry {
                tx: Arc::clone(&tx),
                status: TransactionStatus::Pending,
                added_at: self.now,
                cross_shard,
            },
        );
        tracing::info!(tx_hash = ?hash, pool_size = self.pool.len(), "Transaction added to mempool via submit");

        // Note: Broadcasting is handled by NodeStateMachine which broadcasts to all
        // involved shards. Mempool just manages state.
        vec![Action::EmitTransactionStatus {
            tx_hash: hash,
            status: TransactionStatus::Pending,
            added_at: self.now,
            cross_shard,
        }]
    }

    /// Handle transaction submission from client.
    #[instrument(skip(self, tx), fields(tx_hash = ?tx.hash()))]
    pub fn on_submit_transaction(&mut self, tx: RoutableTransaction) -> Vec<Action> {
        self.on_submit_transaction_arc(Arc::new(tx))
    }

    /// Handle transaction received via gossip.
    #[instrument(skip(self, tx), fields(tx_hash = ?tx.hash()))]
    pub fn on_transaction_gossip_arc(&mut self, tx: Arc<RoutableTransaction>) -> Vec<Action> {
        let hash = tx.hash();

        // Ignore if already have it or if tombstoned (completed/aborted/retried)
        if self.pool.contains_key(&hash) || self.is_tombstoned(&hash) {
            return vec![];
        }

        let cross_shard = tx.is_cross_shard(self.topology.num_shards());
        self.pool.insert(
            hash,
            PoolEntry {
                tx,
                status: TransactionStatus::Pending,
                added_at: self.now,
                cross_shard,
            },
        );
        tracing::debug!(tx_hash = ?hash, pool_size = self.pool.len(), "Transaction added to mempool via gossip");

        // Note: We don't emit TransactionAccepted as an event - it was purely informational
        // and would flood the consensus channel under high transaction load.
        vec![]
    }

    /// Handle transaction received via gossip.
    #[instrument(skip(self, tx), fields(tx_hash = ?tx.hash()))]
    pub fn on_transaction_gossip(&mut self, tx: RoutableTransaction) -> Vec<Action> {
        self.on_transaction_gossip_arc(Arc::new(tx))
    }

    /// Broadcast a transaction to all shards involved in it.
    ///
    /// Uses topology to determine which shards need to receive the transaction
    /// based on its declared reads and writes.
    fn broadcast_to_transaction_shards(&self, tx: &Arc<RoutableTransaction>) -> Vec<Action> {
        let shards = self.topology.all_shards_for_transaction(tx.as_ref());
        let gossip = hyperscale_messages::TransactionGossip::from_arc(Arc::clone(tx));

        shards
            .into_iter()
            .map(|shard| Action::BroadcastToShard {
                shard,
                message: OutboundMessage::TransactionGossip(Box::new(gossip.clone())),
            })
            .collect()
    }

    /// Evict a transaction that has reached a terminal state.
    ///
    /// This removes the transaction from the pool and adds it to the tombstone set
    /// to prevent it from being re-added via gossip. Terminal states include:
    /// - Completed (certificate committed)
    /// - Aborted (explicitly aborted)
    /// - Retried (replaced by a new transaction)
    fn evict_terminal(&mut self, tx_hash: Hash) {
        // Remove locked nodes if this transaction was holding locks
        let tx_to_unlock = self.pool.get(&tx_hash).and_then(|entry| {
            if entry.status.holds_state_lock() {
                Some(Arc::clone(&entry.tx))
            } else {
                None
            }
        });
        if let Some(tx) = tx_to_unlock {
            self.remove_locked_nodes(&tx);
        }
        self.pool.remove(&tx_hash);
        self.tombstones.insert(tx_hash, self.current_height);
    }

    /// Check if a transaction hash is tombstoned (reached terminal state).
    pub fn is_tombstoned(&self, tx_hash: &Hash) -> bool {
        self.tombstones.contains_key(tx_hash)
    }

    /// Process a committed block - update statuses and trigger retries.
    ///
    /// This handles:
    /// 1. Mark committed transactions
    /// 2. Process deferrals → update status to Blocked
    /// 3. Process certificates → mark completed, trigger retries for blocked TXs
    /// 4. Process aborts → update status to terminal
    #[instrument(skip(self, block), fields(
        height = block.header.height.0,
        tx_count = block.transactions.len()
    ))]
    pub fn on_block_committed_full(&mut self, block: &Block) -> Vec<Action> {
        let height = block.header.height;
        let mut actions = Vec::new();

        // Track current height for retry creation
        self.current_height = height;

        // Ensure all committed transactions are in the mempool.
        // This handles the case where we fetched transactions to vote on a block
        // but didn't receive them via gossip. We need them in the mempool for
        // status tracking (deferrals, retries, execution status updates).
        for tx in &block.transactions {
            let hash = tx.hash();
            if !self.pool.contains_key(&hash) {
                let cross_shard = tx.is_cross_shard(self.topology.num_shards());
                self.pool.insert(
                    hash,
                    PoolEntry {
                        tx: Arc::clone(tx),
                        status: TransactionStatus::Pending, // Will be updated by execution
                        added_at: self.now,
                        cross_shard,
                    },
                );
                tracing::debug!(
                    tx_hash = ?hash,
                    height = height.0,
                    "Added committed transaction to mempool"
                );

                // Check if this transaction has a pending retry (winner cert arrived first).
                // If so, immediately create the retry transaction.
                if let Some((winner_hash, cert_height)) = self.pending_retries.remove(&hash) {
                    tracing::info!(
                        tx_hash = %hash,
                        winner = %winner_hash,
                        "Processing pending retry for transaction that arrived after winner certificate"
                    );
                    actions.extend(self.create_retry_for_transaction(
                        Arc::clone(tx),
                        hash,
                        winner_hash,
                        cert_height,
                    ));
                }
            }
        }

        // 1. Update transaction status to Committed and add locks.
        // This must happen synchronously to prevent the same transactions from being
        // re-proposed before the status update is processed. The execution state machine
        // also emits TransactionStatusChanged events, but those go through an async channel
        // that may not be processed before the next proposal.
        for tx in &block.transactions {
            let hash = tx.hash();
            if let Some(entry) = self.pool.get_mut(&hash) {
                // Only update if still Pending (avoid overwriting later states during sync)
                if matches!(entry.status, TransactionStatus::Pending) {
                    let added_at = entry.added_at;
                    let cross_shard = entry.cross_shard;
                    entry.status = TransactionStatus::Committed(height);
                    // Add locks for committed transactions
                    self.add_locked_nodes(tx);
                    actions.push(Action::EmitTransactionStatus {
                        tx_hash: hash,
                        status: TransactionStatus::Committed(height),
                        added_at,
                        cross_shard,
                    });
                }
            }
        }

        // 2. Process deferrals - update status to Blocked
        for deferral in &block.deferred {
            actions.extend(self.on_deferral_committed(deferral.tx_hash, &deferral.reason, height));
        }

        // 3. Process certificates - mark completed, trigger retries
        for cert in &block.committed_certificates {
            actions.extend(self.on_certificate_committed(
                cert.transaction_hash,
                cert.decision,
                height,
            ));
        }

        // 4. Process aborts - mark as aborted with reason and evict
        for abort in &block.aborted {
            if let Some(entry) = self.pool.get(&abort.tx_hash) {
                let added_at = entry.added_at;
                let cross_shard = entry.cross_shard;
                let status = TransactionStatus::Aborted {
                    reason: abort.reason.clone(),
                };
                actions.push(Action::EmitTransactionStatus {
                    tx_hash: abort.tx_hash,
                    status,
                    added_at,
                    cross_shard,
                });
                // Evict from pool and tombstone - terminal state
                self.evict_terminal(abort.tx_hash);
            }
        }

        actions
    }

    /// Handle a deferral committed in a block.
    ///
    /// Updates the deferred TX's status to Blocked and tracks it for retry.
    /// If the transaction is not yet in the pool (sync scenario), stores the
    /// deferral for processing when the transaction arrives.
    fn on_deferral_committed(
        &mut self,
        tx_hash: Hash,
        reason: &DeferReason,
        height: BlockHeight,
    ) -> Vec<Action> {
        let DeferReason::LivelockCycle { winner_tx_hash } = reason;

        // Check if the winner has already completed - if so, create retry immediately.
        // This handles the race condition where the deferral arrives after the winner
        // has already been executed and its certificate committed.
        if let Some(&cert_height) = self.completed_winners.get(winner_tx_hash) {
            if let Some(entry) = self.pool.get(&tx_hash) {
                // Loser is in pool and winner already completed - create retry immediately
                tracing::info!(
                    tx_hash = %tx_hash,
                    winner = %winner_tx_hash,
                    "Deferral arrived after winner completed - creating retry immediately"
                );
                let loser_tx = Arc::clone(&entry.tx);
                return self.create_retry_for_transaction(
                    loser_tx,
                    tx_hash,
                    *winner_tx_hash,
                    cert_height,
                );
            } else {
                // Loser not in pool yet but winner already completed - store for later retry
                tracing::debug!(
                    tx_hash = %tx_hash,
                    winner = %winner_tx_hash,
                    "Deferral arrived after winner completed, loser not in pool - storing for later retry"
                );
                self.pending_retries
                    .insert(tx_hash, (*winner_tx_hash, cert_height));
                return vec![];
            }
        }

        // Get the transaction and update its status
        if let Some(entry) = self.pool.get_mut(&tx_hash) {
            // Update status to Blocked
            let new_status = TransactionStatus::Blocked {
                by: *winner_tx_hash,
            };
            if entry.status.can_transition_to(&new_status) {
                tracing::info!(
                    tx_hash = %tx_hash,
                    winner = %winner_tx_hash,
                    from = %entry.status,
                    "Transaction deferred due to livelock cycle"
                );
                let cross_shard = entry.cross_shard;
                entry.status = new_status.clone();

                // Track for retry when winner completes
                self.blocked_by
                    .insert(tx_hash, (Arc::clone(&entry.tx), *winner_tx_hash));

                // Maintain reverse index for O(1) lookup
                self.blocked_losers_by_winner
                    .entry(*winner_tx_hash)
                    .or_default()
                    .push(tx_hash);

                return vec![Action::EmitTransactionStatus {
                    tx_hash,
                    status: new_status,
                    added_at: entry.added_at,
                    cross_shard,
                }];
            }
        } else {
            // Transaction not in pool yet - this can happen during sync when
            // processing blocks where the deferral references a transaction
            // from an earlier block. Store for later processing.
            tracing::debug!(
                tx_hash = %tx_hash,
                winner = %winner_tx_hash,
                height = height.0,
                "Storing pending deferral for transaction not yet in pool"
            );
            self.pending_deferrals
                .insert(tx_hash, (*winner_tx_hash, height));

            // Also set up the reverse index now, so that if the winner's certificate
            // arrives before the deferred transaction, we know there's a pending loser
            self.blocked_losers_by_winner
                .entry(*winner_tx_hash)
                .or_default()
                .push(tx_hash);
        }

        vec![]
    }

    /// Handle a certificate committed in a block.
    ///
    /// Marks the transaction as completed and triggers retries for any TXs blocked by it.
    fn on_certificate_committed(
        &mut self,
        tx_hash: Hash,
        decision: TransactionDecision,
        height: BlockHeight,
    ) -> Vec<Action> {
        let mut actions = Vec::new();

        // Mark the certificate's TX as completed with the final decision and evict
        if let Some(entry) = self.pool.get(&tx_hash) {
            let added_at = entry.added_at;
            let cross_shard = entry.cross_shard;
            actions.push(Action::EmitTransactionStatus {
                tx_hash,
                status: TransactionStatus::Completed(decision),
                added_at,
                cross_shard,
            });
            // Evict from pool and tombstone - terminal state
            self.evict_terminal(tx_hash);
        }

        // Track this winner as completed so late-arriving deferrals can create retries immediately
        self.completed_winners.insert(tx_hash, height);

        // Check if any blocked TXs were waiting for this winner using reverse index (O(1) lookup)
        let loser_hashes = self
            .blocked_losers_by_winner
            .remove(&tx_hash)
            .unwrap_or_default();

        for loser_hash in loser_hashes {
            // First check if the loser is in blocked_by (normal case - tx was in pool when deferred)
            if let Some((loser_tx, winner_hash)) = self.blocked_by.remove(&loser_hash) {
                // Create retry transaction
                let retry_tx = loser_tx.create_retry(winner_hash, height);
                let retry_hash = retry_tx.hash();

                tracing::info!(
                    original = %loser_hash,
                    retry = %retry_hash,
                    winner = %winner_hash,
                    retry_count = retry_tx.retry_count(),
                    "Creating retry for deferred transaction"
                );

                // Update original's status to Retried and evict
                if let Some(entry) = self.pool.get(&loser_hash) {
                    let added_at = entry.added_at;
                    let cross_shard = entry.cross_shard;
                    actions.push(Action::EmitTransactionStatus {
                        tx_hash: loser_hash,
                        status: TransactionStatus::Retried { new_tx: retry_hash },
                        added_at,
                        cross_shard,
                    });
                    // Evict from pool and tombstone - terminal state
                    self.evict_terminal(loser_hash);
                }

                // Add retry to mempool if not already present (dedup by hash)
                if !self.pool.contains_key(&retry_hash) && !self.is_tombstoned(&retry_hash) {
                    let retry_tx = Arc::new(retry_tx);
                    let cross_shard = retry_tx.is_cross_shard(self.topology.num_shards());
                    self.pool.insert(
                        retry_hash,
                        PoolEntry {
                            tx: Arc::clone(&retry_tx),
                            status: TransactionStatus::Pending,
                            added_at: self.now,
                            cross_shard,
                        },
                    );

                    // Emit status for retry transaction
                    actions.push(Action::EmitTransactionStatus {
                        tx_hash: retry_hash,
                        status: TransactionStatus::Pending,
                        added_at: self.now,
                        cross_shard,
                    });

                    // Gossip the retry to relevant shards
                    actions.extend(self.broadcast_to_transaction_shards(&retry_tx));
                }
            } else if let Some((winner_hash, _deferral_height)) =
                self.pending_deferrals.remove(&loser_hash)
            {
                // The loser was deferred but wasn't in the pool yet (sync scenario).
                // The winner's certificate arrived before the loser transaction.
                // We can't create a retry yet because we don't have the loser transaction.
                // Store in a new structure to create retry when the loser arrives.
                tracing::debug!(
                    loser = %loser_hash,
                    winner = %winner_hash,
                    "Winner certificate arrived before deferred loser transaction - storing for later retry"
                );
                self.pending_retries
                    .insert(loser_hash, (winner_hash, height));
            }
        }

        actions
    }

    /// Create a retry transaction for a deferred loser.
    ///
    /// This is extracted into a helper to handle both:
    /// 1. Normal case: winner certificate arrives, loser is in pool
    /// 2. Sync case: loser transaction arrives after winner certificate
    fn create_retry_for_transaction(
        &mut self,
        loser_tx: Arc<RoutableTransaction>,
        loser_hash: Hash,
        winner_hash: Hash,
        height: BlockHeight,
    ) -> Vec<Action> {
        let mut actions = Vec::new();

        // Create retry transaction
        let retry_tx = loser_tx.create_retry(winner_hash, height);
        let retry_hash = retry_tx.hash();

        tracing::info!(
            original = %loser_hash,
            retry = %retry_hash,
            winner = %winner_hash,
            retry_count = retry_tx.retry_count(),
            "Creating retry for deferred transaction"
        );

        // Update original's status to Retried and evict
        if let Some(entry) = self.pool.get(&loser_hash) {
            let added_at = entry.added_at;
            let cross_shard = entry.cross_shard;
            actions.push(Action::EmitTransactionStatus {
                tx_hash: loser_hash,
                status: TransactionStatus::Retried { new_tx: retry_hash },
                added_at,
                cross_shard,
            });
            // Evict from pool and tombstone - terminal state
            self.evict_terminal(loser_hash);
        }

        // Add retry to mempool if not already present (dedup by hash)
        if !self.pool.contains_key(&retry_hash) && !self.is_tombstoned(&retry_hash) {
            let retry_tx = Arc::new(retry_tx);
            let cross_shard = retry_tx.is_cross_shard(self.topology.num_shards());
            self.pool.insert(
                retry_hash,
                PoolEntry {
                    tx: Arc::clone(&retry_tx),
                    status: TransactionStatus::Pending,
                    added_at: self.now,
                    cross_shard,
                },
            );

            // Emit status for retry transaction
            actions.push(Action::EmitTransactionStatus {
                tx_hash: retry_hash,
                status: TransactionStatus::Pending,
                added_at: self.now,
                cross_shard,
            });

            // Gossip the retry to relevant shards
            actions.extend(self.broadcast_to_transaction_shards(&retry_tx));
        }

        actions
    }

    /// Mark transactions as committed when block is committed (legacy method).
    #[deprecated(note = "Use on_block_committed_full instead")]
    pub fn on_block_committed(&mut self, tx_hashes: &[Hash], height: BlockHeight) {
        for hash in tx_hashes {
            // Check if we need to add locked nodes (clone tx first to avoid borrow issues)
            let should_add_locks = self
                .pool
                .get(hash)
                .is_some_and(|entry| !entry.status.holds_state_lock());
            let tx_clone = self.pool.get(hash).map(|e| Arc::clone(&e.tx));

            if should_add_locks {
                if let Some(tx) = tx_clone {
                    self.add_locked_nodes(&tx);
                }
            }

            if let Some(entry) = self.pool.get_mut(hash) {
                entry.status = TransactionStatus::Committed(height);
            }
        }
    }

    /// Mark a transaction as executed (execution complete, certificate created).
    ///
    /// Called when ExecutionState creates a TransactionCertificate.
    /// Also triggers retries for any transactions blocked by this winner.
    #[instrument(skip(self), fields(tx_hash = ?tx_hash, accepted = accepted))]
    pub fn on_transaction_executed(&mut self, tx_hash: Hash, accepted: bool) -> Vec<Action> {
        let mut actions = Vec::new();

        if let Some(entry) = self.pool.get_mut(&tx_hash) {
            let decision = if accepted {
                TransactionDecision::Accept
            } else {
                TransactionDecision::Reject
            };
            let added_at = entry.added_at;
            let cross_shard = entry.cross_shard;
            entry.status = TransactionStatus::Executed(decision);
            actions.push(Action::EmitTransactionStatus {
                tx_hash,
                status: TransactionStatus::Executed(decision),
                added_at,
                cross_shard,
            });
        }

        // Check if any blocked transactions were waiting for this winner to complete.
        // This triggers retries immediately when the winner executes, rather than
        // waiting for the certificate to be committed in a block.
        // Using reverse index for O(1) lookup
        let loser_hashes = self
            .blocked_losers_by_winner
            .remove(&tx_hash)
            .unwrap_or_default();

        let height = self.current_height;
        for loser_hash in loser_hashes {
            // Get the loser transaction from blocked_by
            let Some((loser_tx, winner_hash)) = self.blocked_by.remove(&loser_hash) else {
                continue;
            };
            // Create retry transaction
            let retry_tx = loser_tx.create_retry(winner_hash, height);
            let retry_hash = retry_tx.hash();

            tracing::info!(
                original = %loser_hash,
                retry = %retry_hash,
                winner = %winner_hash,
                retry_count = retry_tx.retry_count(),
                "Creating retry for blocked transaction (winner finalized)"
            );

            // Update original's status to Retried and evict
            if let Some(entry) = self.pool.get(&loser_hash) {
                let added_at = entry.added_at;
                let cross_shard = entry.cross_shard;
                actions.push(Action::EmitTransactionStatus {
                    tx_hash: loser_hash,
                    status: TransactionStatus::Retried { new_tx: retry_hash },
                    added_at,
                    cross_shard,
                });
                // Evict from pool and tombstone - terminal state
                self.evict_terminal(loser_hash);
            }

            // Add retry to mempool if not already present (dedup by hash)
            if !self.pool.contains_key(&retry_hash) && !self.is_tombstoned(&retry_hash) {
                let retry_tx = Arc::new(retry_tx);
                let cross_shard = retry_tx.is_cross_shard(self.topology.num_shards());
                self.pool.insert(
                    retry_hash,
                    PoolEntry {
                        tx: Arc::clone(&retry_tx),
                        status: TransactionStatus::Pending,
                        added_at: self.now,
                        cross_shard,
                    },
                );

                // Emit status for retry transaction
                actions.push(Action::EmitTransactionStatus {
                    tx_hash: retry_hash,
                    status: TransactionStatus::Pending,
                    added_at: self.now,
                    cross_shard,
                });

                // Gossip the retry to relevant shards
                actions.extend(self.broadcast_to_transaction_shards(&retry_tx));
            }
        }

        actions
    }

    /// Mark a transaction as completed (certificate committed in block).
    ///
    /// This is a terminal state - the transaction is evicted from mempool.
    pub fn mark_completed(&mut self, tx_hash: &Hash, decision: TransactionDecision) -> Vec<Action> {
        if let Some(entry) = self.pool.get(tx_hash) {
            let added_at = entry.added_at;
            let cross_shard = entry.cross_shard;
            // Evict from pool and tombstone - terminal state
            self.evict_terminal(*tx_hash);
            return vec![Action::EmitTransactionStatus {
                tx_hash: *tx_hash,
                status: TransactionStatus::Completed(decision),
                added_at,
                cross_shard,
            }];
        }
        vec![]
    }

    /// Update transaction status to a new state.
    ///
    /// This is used by the execution state machine to update status during
    /// the transaction lifecycle (Committed, Executed, etc.).
    ///
    /// Returns an action to emit the status update if the transition was valid.
    pub fn update_status(&mut self, tx_hash: &Hash, new_status: TransactionStatus) -> Vec<Action> {
        if let Some(entry) = self.pool.get_mut(tx_hash) {
            // Case 1: Idempotent update - already in the target state
            if entry.status == new_status {
                tracing::trace!(
                    tx_hash = ?tx_hash,
                    status = %entry.status,
                    "Ignoring duplicate status update"
                );
                return vec![];
            }

            // Case 2: Valid transition - apply it
            if entry.status.can_transition_to(&new_status) {
                tracing::debug!(
                    tx_hash = ?tx_hash,
                    from = %entry.status,
                    to = %new_status,
                    "Transaction status transition"
                );

                // Update locked nodes cache based on status transition
                let was_holding = entry.status.holds_state_lock();
                let will_hold = new_status.holds_state_lock();
                let tx = Arc::clone(&entry.tx);
                let cross_shard = entry.cross_shard;

                if !was_holding && will_hold {
                    // Acquiring lock
                    self.add_locked_nodes(&tx);
                } else if was_holding && !will_hold {
                    // Releasing lock (shouldn't happen - locks held until terminal)
                    self.remove_locked_nodes(&tx);
                }

                // Re-borrow entry after calling helper methods
                let entry = self.pool.get_mut(tx_hash).unwrap();
                let added_at = entry.added_at;
                entry.status = new_status.clone();
                return vec![Action::EmitTransactionStatus {
                    tx_hash: *tx_hash,
                    status: new_status,
                    added_at,
                    cross_shard,
                }];
            }

            // Case 3: Invalid transition - determine if stale or truly invalid
            if new_status.ordinal() < entry.status.ordinal() {
                // Stale update: we've already progressed past this state.
                // This can happen due to message reordering in distributed systems.
                tracing::debug!(
                    tx_hash = ?tx_hash,
                    current = %entry.status,
                    stale = %new_status,
                    "Ignoring stale status update (already progressed past this state)"
                );
            } else {
                // Truly invalid transition - this indicates a bug in the state machine
                tracing::warn!(
                    tx_hash = ?tx_hash,
                    from = %entry.status,
                    to = %new_status,
                    "Invalid transaction status transition"
                );
            }
        }
        vec![]
    }

    /// Add a transaction's nodes to the locked set.
    /// Called when a transaction transitions TO a lock-holding state (Committed/Executed).
    fn add_locked_nodes(&mut self, tx: &RoutableTransaction) {
        for node in tx.all_declared_nodes() {
            self.locked_nodes_cache.insert(*node);
        }
    }

    /// Remove a transaction's nodes from the locked set.
    /// Called when a transaction transitions FROM a lock-holding state (evicted).
    fn remove_locked_nodes(&mut self, tx: &RoutableTransaction) {
        for node in tx.all_declared_nodes() {
            self.locked_nodes_cache.remove(node);
        }
    }

    /// Check if a transaction conflicts with any locked nodes.
    /// Uses the cached locked_nodes_cache for O(k) lookup where k = nodes in tx.
    fn conflicts_with_locked(&self, tx: &RoutableTransaction) -> bool {
        tx.all_declared_nodes()
            .any(|node| self.locked_nodes_cache.contains(node))
    }

    /// Get transactions ready for inclusion in a block.
    ///
    /// Returns transactions that are:
    /// 1. In Pending status (not yet accepted)
    /// 2. Do not conflict with any in-flight transaction (no shared nodes)
    /// 3. Sorted by hash (ascending) to reduce cross-shard conflicts
    ///
    /// The hash-ordering ensures different shards are more likely to pick
    /// the same transactions, reducing cycle formation in cross-shard execution.
    ///
    /// Since pool is a BTreeMap sorted by hash, we iterate in order and
    /// can take early once we have max_count results - no sorting needed.
    pub fn ready_transactions(&self, max_count: usize) -> Vec<Arc<RoutableTransaction>> {
        // BTreeMap iterates in key (hash) order, so no sorting needed
        // Filter for pending, non-conflicting transactions, take up to max_count
        self.pool
            .values()
            .filter(|e| e.status == TransactionStatus::Pending)
            .filter(|e| !self.conflicts_with_locked(&e.tx))
            .take(max_count)
            .map(|e| Arc::clone(&e.tx))
            .collect()
    }

    /// Get lock contention statistics.
    ///
    /// Returns counts of:
    /// - `locked_nodes`: Number of nodes currently locked by in-flight transactions
    /// - `blocked_count`: Number of transactions blocked waiting for a winner
    /// - `pending_count`: Number of transactions in Pending status
    /// - `pending_blocked`: Number of pending transactions that conflict with locked nodes
    /// - `committed_count`: Number of transactions in Committed status
    /// - `executed_count`: Number of transactions in Executed status
    pub fn lock_contention_stats(&self) -> LockContentionStats {
        let locked_nodes = self.locked_nodes_cache.len() as u64;
        let blocked_count = self.blocked_by.len() as u64;

        // Single pass over pool to count transactions by status
        let (pending_count, pending_blocked, committed_count, executed_count) =
            self.pool.values().fold(
                (0u64, 0u64, 0u64, 0u64),
                |(pending, pending_blocked, committed, executed), e| match &e.status {
                    TransactionStatus::Pending => {
                        let is_blocked = self.conflicts_with_locked(&e.tx);
                        (
                            pending + 1,
                            pending_blocked + is_blocked as u64,
                            committed,
                            executed,
                        )
                    }
                    TransactionStatus::Committed(_) => {
                        (pending, pending_blocked, committed + 1, executed)
                    }
                    TransactionStatus::Executed(_) => {
                        (pending, pending_blocked, committed, executed + 1)
                    }
                    _ => (pending, pending_blocked, committed, executed),
                },
            );

        LockContentionStats {
            locked_nodes,
            blocked_count,
            pending_count,
            pending_blocked,
            committed_count,
            executed_count,
        }
    }

    /// Check if we have a transaction.
    pub fn has_transaction(&self, hash: &Hash) -> bool {
        self.pool.contains_key(hash)
    }

    /// Get a transaction Arc by hash.
    pub fn get_transaction(&self, hash: &Hash) -> Option<Arc<RoutableTransaction>> {
        self.pool.get(hash).map(|e| Arc::clone(&e.tx))
    }

    /// Get transaction status.
    pub fn status(&self, hash: &Hash) -> Option<TransactionStatus> {
        self.pool.get(hash).map(|e| e.status.clone())
    }

    /// Get all transactions as a HashMap (for block header validation).
    ///
    /// This allows BFT to look up transactions by hash when receiving block headers.
    pub fn transactions_by_hash(&self) -> HashMap<Hash, Arc<RoutableTransaction>> {
        self.pool
            .iter()
            .map(|(hash, entry)| (*hash, Arc::clone(&entry.tx)))
            .collect()
    }

    /// Get the number of transactions in the pool.
    pub fn len(&self) -> usize {
        self.pool.len()
    }

    /// Get the mempool as a hash map for BFT pending block completion.
    pub fn as_hash_map(&self) -> std::collections::HashMap<Hash, Arc<RoutableTransaction>> {
        self.pool
            .iter()
            .map(|(hash, entry)| (*hash, Arc::clone(&entry.tx)))
            .collect()
    }

    /// Check if the pool is empty.
    pub fn is_empty(&self) -> bool {
        self.pool.is_empty()
    }

    /// Check if the mempool is accepting new RPC transactions.
    ///
    /// Returns `false` if the pool has reached `max_rpc_pool_size`, meaning
    /// new RPC submissions should be rejected with backpressure.
    ///
    /// Note: This does NOT affect gossip transactions, which are always accepted
    /// because other nodes may propose blocks containing them.
    pub fn is_accepting_rpc_transactions(&self) -> bool {
        match self.config.max_rpc_pool_size {
            Some(max) => self.pool.len() < max,
            None => true,
        }
    }

    /// Get the maximum RPC pool size, if configured.
    pub fn max_rpc_pool_size(&self) -> Option<usize> {
        self.config.max_rpc_pool_size
    }

    /// Get all incomplete transactions (not yet finalized or completed).
    ///
    /// Returns tuples of (hash, status, transaction Arc) for analysis.
    pub fn incomplete_transactions(
        &self,
    ) -> Vec<(Hash, TransactionStatus, Arc<RoutableTransaction>)> {
        self.pool
            .iter()
            .filter(|(_, entry)| {
                !matches!(
                    entry.status,
                    TransactionStatus::Executed(_) | TransactionStatus::Completed(_)
                )
            })
            .map(|(hash, entry)| (*hash, entry.status.clone(), Arc::clone(&entry.tx)))
            .collect()
    }

    /// Get transactions that have timed out waiting for execution.
    ///
    /// Transactions timeout if they've been holding state locks for too long.
    /// This is a safety net for N-way cycles that aren't detected by pairwise
    /// cycle detection.
    ///
    /// Returns `TransactionAbort` entries ready for inclusion in a block.
    ///
    /// # Parameters
    /// - `current_height`: The current block height
    /// - `timeout_blocks`: Number of blocks after which a TX is considered timed out
    /// - `max_retries`: Maximum retry count before aborting
    pub fn get_timed_out_transactions(
        &self,
        current_height: BlockHeight,
        timeout_blocks: u64,
        max_retries: u32,
    ) -> Vec<TransactionAbort> {
        let mut aborts = Vec::new();

        for (hash, entry) in &self.pool {
            // Skip transactions that are already finalized or completed
            if matches!(
                entry.status,
                TransactionStatus::Executed(_) | TransactionStatus::Completed(_)
            ) {
                continue;
            }

            // Check for execution timeout (TX stuck in lock-holding state too long)
            if let TransactionStatus::Committed(committed_at) = &entry.status {
                let blocks_elapsed = current_height.0.saturating_sub(committed_at.0);
                if blocks_elapsed >= timeout_blocks {
                    tracing::debug!(
                        tx_hash = %hash,
                        committed_at = committed_at.0,
                        current_height = current_height.0,
                        blocks_elapsed = blocks_elapsed,
                        "Transaction timed out waiting for execution"
                    );
                    aborts.push(TransactionAbort {
                        tx_hash: *hash,
                        reason: AbortReason::ExecutionTimeout {
                            committed_at: *committed_at,
                        },
                        block_height: BlockHeight(0), // Filled in by proposer
                    });
                }
            }

            // Note: Executed transactions also hold locks but don't have a
            // committed height embedded. For simplicity, we only timeout from
            // Committed state. Executed→Completed should happen quickly anyway.

            // Check for too many retries
            if entry.tx.exceeds_max_retries(max_retries) {
                tracing::info!(
                    tx_hash = %hash,
                    retry_count = entry.tx.retry_count(),
                    max_retries = max_retries,
                    "Transaction exceeded maximum retry count"
                );
                aborts.push(TransactionAbort {
                    tx_hash: *hash,
                    reason: AbortReason::TooManyRetries {
                        retry_count: entry.tx.retry_count(),
                    },
                    block_height: BlockHeight(0), // Filled in by proposer
                });
            }
        }

        aborts
    }

    /// Clean up old tombstones and completed winners to prevent unbounded memory growth.
    ///
    /// Tombstones are kept for `retention_blocks` after creation to ensure gossip
    /// propagation has completed. After that, they can be safely removed since any
    /// late-arriving gossip for a very old transaction is likely stale anyway.
    ///
    /// # Parameters
    /// - `current_height`: The current block height
    /// - `retention_blocks`: Number of blocks to retain tombstones after creation
    ///
    /// # Returns
    /// Number of tombstones cleaned up
    pub fn cleanup_old_tombstones(
        &mut self,
        current_height: BlockHeight,
        retention_blocks: u64,
    ) -> usize {
        let cutoff = current_height.0.saturating_sub(retention_blocks);
        let before_count = self.tombstones.len();

        self.tombstones.retain(|_, height| height.0 > cutoff);

        // Also clean up completed_winners using the same retention policy
        self.completed_winners.retain(|_, height| height.0 > cutoff);

        before_count - self.tombstones.len()
    }

    /// Get the number of tombstones currently tracked.
    pub fn tombstone_count(&self) -> usize {
        self.tombstones.len()
    }
}

impl SubStateMachine for MempoolState {
    fn try_handle(&mut self, event: &Event) -> Option<Vec<Action>> {
        match event {
            Event::SubmitTransaction { tx } => Some(self.on_submit_transaction_arc(Arc::clone(tx))),
            Event::TransactionGossipReceived { tx } => {
                Some(self.on_transaction_gossip_arc(Arc::clone(tx)))
            }
            Event::BlockCommitted { block, .. } => {
                // Process block fully including deferrals, certificates, and aborts
                Some(self.on_block_committed_full(block))
            }
            Event::TransactionExecuted { tx_hash, accepted } => {
                Some(self.on_transaction_executed(*tx_hash, *accepted))
            }
            // Handle status update events from execution
            Event::TransactionStatusChanged { tx_hash, status } => {
                Some(self.update_status(tx_hash, status.clone()))
            }
            _ => None,
        }
    }

    fn set_time(&mut self, now: Duration) {
        self.now = now;
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use hyperscale_types::{
        test_utils::test_transaction, Block, BlockHeader, DeferReason, KeyPair, QuorumCertificate,
        StaticTopology, TransactionCertificate, TransactionDefer, ValidatorId, ValidatorInfo,
        ValidatorSet,
    };
    use std::collections::BTreeMap;

    fn make_test_topology() -> Arc<dyn Topology> {
        let validators: Vec<_> = (0..4)
            .map(|i| ValidatorInfo {
                validator_id: ValidatorId(i),
                public_key: KeyPair::generate_ed25519().public_key(),
                voting_power: 1,
            })
            .collect();
        let validator_set = ValidatorSet::new(validators);
        Arc::new(StaticTopology::new(ValidatorId(0), 1, validator_set))
    }

    fn make_test_block(
        height: u64,
        transactions: Vec<RoutableTransaction>,
        deferred: Vec<TransactionDefer>,
        certificates: Vec<TransactionCertificate>,
        aborted: Vec<TransactionAbort>,
    ) -> Block {
        Block {
            header: BlockHeader {
                height: BlockHeight(height),
                parent_hash: Hash::from_bytes(b"parent"),
                parent_qc: QuorumCertificate::genesis(),
                proposer: ValidatorId(0),
                timestamp: 1234567890,
                round: 0,
                is_fallback: false,
            },
            transactions: transactions.into_iter().map(Arc::new).collect(),
            committed_certificates: certificates.into_iter().map(Arc::new).collect(),
            deferred,
            aborted,
        }
    }

    fn make_test_certificate(tx_hash: Hash) -> TransactionCertificate {
        TransactionCertificate {
            transaction_hash: tx_hash,
            decision: TransactionDecision::Accept,
            shard_proofs: BTreeMap::new(), // Empty for test - just need tx_hash
        }
    }

    #[test]
    fn test_deferral_updates_status_to_blocked() {
        let mut mempool = MempoolState::new(make_test_topology());

        // Create and add a transaction
        let tx = test_transaction(1);
        let tx_hash = tx.hash();
        mempool.on_submit_transaction(tx.clone());

        // Commit the transaction first (deferrals apply to committed TXs)
        let commit_block = make_test_block(1, vec![tx.clone()], vec![], vec![], vec![]);
        mempool.on_block_committed_full(&commit_block);

        // Simulate the TransactionStatusChanged event from execution
        mempool.update_status(&tx_hash, TransactionStatus::Committed(BlockHeight(1)));

        // Verify status is Committed
        assert!(matches!(
            mempool.status(&tx_hash),
            Some(TransactionStatus::Committed(_))
        ));

        // Create another TX as the "winner"
        let winner_tx = test_transaction(2);
        let winner_hash = winner_tx.hash();
        mempool.on_submit_transaction(winner_tx.clone());

        // Create a deferral for our TX
        let deferral = TransactionDefer {
            tx_hash,
            reason: DeferReason::LivelockCycle {
                winner_tx_hash: winner_hash,
            },
            block_height: BlockHeight(2),
        };

        // Process block with deferral
        let defer_block = make_test_block(2, vec![], vec![deferral], vec![], vec![]);
        mempool.on_block_committed_full(&defer_block);

        // Verify status is now Blocked
        let status = mempool.status(&tx_hash);
        assert!(matches!(
            status,
            Some(TransactionStatus::Blocked { by }) if by == winner_hash
        ));

        // Verify it's tracked in blocked_by
        assert!(mempool.blocked_by.contains_key(&tx_hash));
    }

    #[test]
    fn test_winner_certificate_triggers_retry() {
        let mut mempool = MempoolState::new(make_test_topology());

        // Create loser TX and submit
        let loser_tx = test_transaction(1);
        let loser_hash = loser_tx.hash();
        mempool.on_submit_transaction(loser_tx.clone());

        // Create winner TX and submit
        let winner_tx = test_transaction(2);
        let winner_hash = winner_tx.hash();
        mempool.on_submit_transaction(winner_tx.clone());

        // Commit both
        let commit_block = make_test_block(
            1,
            vec![loser_tx.clone(), winner_tx.clone()],
            vec![],
            vec![],
            vec![],
        );
        mempool.on_block_committed_full(&commit_block);

        // Defer the loser
        let deferral = TransactionDefer {
            tx_hash: loser_hash,
            reason: DeferReason::LivelockCycle {
                winner_tx_hash: winner_hash,
            },
            block_height: BlockHeight(2),
        };
        let defer_block = make_test_block(2, vec![], vec![deferral], vec![], vec![]);
        mempool.on_block_committed_full(&defer_block);

        // Verify loser is blocked
        assert!(matches!(
            mempool.status(&loser_hash),
            Some(TransactionStatus::Blocked { .. })
        ));

        // Winner's certificate commits
        let winner_cert = make_test_certificate(winner_hash);
        let cert_block = make_test_block(3, vec![], vec![], vec![winner_cert], vec![]);
        let actions = mempool.on_block_committed_full(&cert_block);

        // Should have emitted Retried status for loser
        let retried_action = actions.iter().find(|a| {
            matches!(a, Action::EmitTransactionStatus { tx_hash, status: TransactionStatus::Retried { .. }, .. } if *tx_hash == loser_hash)
        });
        assert!(
            retried_action.is_some(),
            "Should have emitted Retried status for loser"
        );

        // Loser should be evicted from pool (terminal state)
        assert!(
            mempool.status(&loser_hash).is_none(),
            "Loser should be evicted from pool after Retried"
        );

        // Extract retry hash from the action
        let retry_hash = match retried_action.unwrap() {
            Action::EmitTransactionStatus {
                status: TransactionStatus::Retried { new_tx },
                ..
            } => *new_tx,
            _ => unreachable!(),
        };

        // Retry should exist in pool as Pending
        let retry_status = mempool.status(&retry_hash);
        assert!(
            matches!(retry_status, Some(TransactionStatus::Pending)),
            "Retry should be Pending, got {:?}",
            retry_status
        );

        // blocked_by should be cleared
        assert!(!mempool.blocked_by.contains_key(&loser_hash));
    }

    #[test]
    fn test_timeout_detection() {
        let mut mempool = MempoolState::new(make_test_topology());

        // Create and commit a TX
        let tx = test_transaction(1);
        let tx_hash = tx.hash();
        mempool.on_submit_transaction(tx.clone());

        let commit_block = make_test_block(1, vec![tx], vec![], vec![], vec![]);
        mempool.on_block_committed_full(&commit_block);

        // Simulate the TransactionStatusChanged event from execution
        mempool.update_status(&tx_hash, TransactionStatus::Committed(BlockHeight(1)));

        // Check for timeouts - not enough blocks elapsed
        let aborts = mempool.get_timed_out_transactions(BlockHeight(20), 30, 3);
        assert!(aborts.is_empty(), "Should not timeout yet");

        // Check for timeouts - now enough blocks
        let aborts = mempool.get_timed_out_transactions(BlockHeight(35), 30, 3);
        assert_eq!(aborts.len(), 1);
        assert_eq!(aborts[0].tx_hash, tx_hash);
        assert!(matches!(
            aborts[0].reason,
            AbortReason::ExecutionTimeout { .. }
        ));
    }

    #[test]
    fn test_too_many_retries_detection() {
        let mut mempool = MempoolState::new(make_test_topology());

        // Create a TX that has already been retried multiple times
        let tx = test_transaction(1);
        let _tx_hash = tx.hash();

        // Manually create a retry TX (simulating previous retries)
        let retry1 = tx.create_retry(Hash::from_bytes(b"winner1"), BlockHeight(1));
        let retry2 = retry1.create_retry(Hash::from_bytes(b"winner2"), BlockHeight(2));
        let retry3 = retry2.create_retry(Hash::from_bytes(b"winner3"), BlockHeight(3));

        assert_eq!(retry3.retry_count(), 3);

        // Submit the multiply-retried TX
        mempool.on_submit_transaction(retry3.clone());

        // Should detect too many retries (max_retries = 3 means 3 retries allowed, 4th would be rejected)
        let aborts = mempool.get_timed_out_transactions(BlockHeight(10), 100, 3);
        assert_eq!(aborts.len(), 1);
        assert!(matches!(
            aborts[0].reason,
            AbortReason::TooManyRetries { retry_count: 3 }
        ));
    }

    #[test]
    fn test_abort_updates_status() {
        let mut mempool = MempoolState::new(make_test_topology());

        // Create and commit a TX
        let tx = test_transaction(1);
        let tx_hash = tx.hash();
        mempool.on_submit_transaction(tx.clone());

        let commit_block = make_test_block(1, vec![tx], vec![], vec![], vec![]);
        mempool.on_block_committed_full(&commit_block);

        // Process an abort
        let abort = TransactionAbort {
            tx_hash,
            reason: AbortReason::ExecutionTimeout {
                committed_at: BlockHeight(1),
            },
            block_height: BlockHeight(35),
        };
        let abort_block = make_test_block(35, vec![], vec![], vec![], vec![abort]);
        let actions = mempool.on_block_committed_full(&abort_block);

        // Should have emitted Aborted status
        let aborted_action = actions.iter().find(|a| {
            matches!(a, Action::EmitTransactionStatus { tx_hash: h, status: TransactionStatus::Aborted { .. }, .. } if *h == tx_hash)
        });
        assert!(
            aborted_action.is_some(),
            "Should have emitted Aborted status"
        );

        // Transaction should be evicted from pool (terminal state)
        assert!(
            mempool.status(&tx_hash).is_none(),
            "Transaction should be evicted from pool after Aborted"
        );
    }

    #[test]
    fn test_retry_has_different_hash() {
        let tx = test_transaction(1);
        let original_hash = tx.hash();

        let retry = tx.create_retry(Hash::from_bytes(b"winner"), BlockHeight(5));
        let retry_hash = retry.hash();

        // Retry must have different hash
        assert_ne!(
            original_hash, retry_hash,
            "Retry must have different hash from original"
        );

        // But same underlying transaction content (declared reads/writes are fields)
        assert_eq!(tx.declared_reads, retry.declared_reads);
        assert_eq!(tx.declared_writes, retry.declared_writes);

        // Retry knows its original
        assert_eq!(retry.original_hash(), original_hash);
        assert_eq!(retry.retry_count(), 1);
    }

    // ═══════════════════════════════════════════════════════════════════════════
    // Sync Scenario Tests
    // ═══════════════════════════════════════════════════════════════════════════

    #[test]
    fn test_sync_deferral_for_transaction_not_in_pool() {
        // Scenario: Node syncs a block containing a deferral for a transaction
        // that was committed in an earlier block the node didn't have.
        // The deferral should be stored and processed when the transaction arrives.

        let mut mempool = MempoolState::new(make_test_topology());

        // Create transactions
        let loser_tx = test_transaction(1);
        let loser_hash = loser_tx.hash();
        let winner_tx = test_transaction(2);
        let winner_hash = winner_tx.hash();

        // Process a synced block that contains ONLY the deferral and certificate
        // (simulating: loser_tx was in an earlier block we don't have yet)
        let deferral = TransactionDefer {
            tx_hash: loser_hash,
            reason: DeferReason::LivelockCycle {
                winner_tx_hash: winner_hash,
            },
            block_height: BlockHeight(5),
        };
        let winner_cert = make_test_certificate(winner_hash);

        // Process block with deferral and certificate, but WITHOUT the loser transaction
        let sync_block = make_test_block(
            5,
            vec![winner_tx],
            vec![deferral],
            vec![winner_cert],
            vec![],
        );
        let _actions = mempool.on_block_committed_full(&sync_block);

        // The loser transaction is NOT in the pool
        assert!(mempool.status(&loser_hash).is_none());

        // But we should have a pending retry stored for it
        assert!(
            mempool.pending_retries.contains_key(&loser_hash),
            "Should have stored pending retry for loser"
        );

        // Now the earlier block arrives with the loser transaction
        let earlier_block = make_test_block(3, vec![loser_tx.clone()], vec![], vec![], vec![]);
        let actions = mempool.on_block_committed_full(&earlier_block);

        // The pending retry should have been processed - we should see retry creation actions
        let retried_action = actions.iter().find(|a| {
            matches!(a, Action::EmitTransactionStatus { tx_hash, status: TransactionStatus::Retried { .. }, .. } if *tx_hash == loser_hash)
        });
        assert!(
            retried_action.is_some(),
            "Should have created retry when deferred transaction arrived"
        );

        // Pending retry should be cleared
        assert!(
            !mempool.pending_retries.contains_key(&loser_hash),
            "Pending retry should be cleared after processing"
        );
    }

    #[test]
    fn test_sync_deferral_before_certificate() {
        // Scenario: Deferral arrives in block N, but loser tx is not in pool yet.
        // Certificate arrives in block N+1.
        // Then loser tx arrives in block N+2.
        // Retry should be created when loser tx arrives.

        let mut mempool = MempoolState::new(make_test_topology());

        let loser_tx = test_transaction(1);
        let loser_hash = loser_tx.hash();
        let winner_tx = test_transaction(2);
        let winner_hash = winner_tx.hash();

        // Block N: Deferral without loser tx in pool
        let deferral = TransactionDefer {
            tx_hash: loser_hash,
            reason: DeferReason::LivelockCycle {
                winner_tx_hash: winner_hash,
            },
            block_height: BlockHeight(5),
        };
        let block_n = make_test_block(5, vec![], vec![deferral], vec![], vec![]);
        mempool.on_block_committed_full(&block_n);

        // Pending deferral should be stored
        assert!(
            mempool.pending_deferrals.contains_key(&loser_hash),
            "Should have stored pending deferral"
        );
        // Reverse index should be set up
        assert!(
            mempool
                .blocked_losers_by_winner
                .get(&winner_hash)
                .is_some_and(|losers| losers.contains(&loser_hash)),
            "Reverse index should contain loser"
        );

        // Block N+1: Winner's certificate arrives
        let winner_cert = make_test_certificate(winner_hash);
        let block_n1 = make_test_block(6, vec![winner_tx], vec![], vec![winner_cert], vec![]);
        mempool.on_block_committed_full(&block_n1);

        // Pending deferral should be converted to pending retry
        assert!(
            !mempool.pending_deferrals.contains_key(&loser_hash),
            "Pending deferral should be removed"
        );
        assert!(
            mempool.pending_retries.contains_key(&loser_hash),
            "Should have stored pending retry"
        );

        // Block N+2: Loser tx finally arrives
        let block_n2 = make_test_block(7, vec![loser_tx.clone()], vec![], vec![], vec![]);
        let actions = mempool.on_block_committed_full(&block_n2);

        // Retry should be created
        let retried_action = actions.iter().find(|a| {
            matches!(a, Action::EmitTransactionStatus { tx_hash, status: TransactionStatus::Retried { .. }, .. } if *tx_hash == loser_hash)
        });
        assert!(
            retried_action.is_some(),
            "Should have created retry when loser tx arrived"
        );

        // All pending structures should be cleared
        assert!(!mempool.pending_deferrals.contains_key(&loser_hash));
        assert!(!mempool.pending_retries.contains_key(&loser_hash));
    }

    #[test]
    fn test_sync_deferral_with_tx_in_same_block() {
        // Scenario: Synced block contains both the transaction AND its deferral.
        // This is the normal case - should work as before.

        let mut mempool = MempoolState::new(make_test_topology());

        let loser_tx = test_transaction(1);
        let loser_hash = loser_tx.hash();
        let winner_tx = test_transaction(2);
        let winner_hash = winner_tx.hash();

        // Block with both loser tx and deferral
        let deferral = TransactionDefer {
            tx_hash: loser_hash,
            reason: DeferReason::LivelockCycle {
                winner_tx_hash: winner_hash,
            },
            block_height: BlockHeight(5),
        };
        let block = make_test_block(
            5,
            vec![loser_tx.clone(), winner_tx],
            vec![deferral],
            vec![],
            vec![],
        );
        let actions = mempool.on_block_committed_full(&block);

        // Transaction should be in pool and blocked
        let status = mempool.status(&loser_hash);
        assert!(
            matches!(status, Some(TransactionStatus::Blocked { by }) if by == winner_hash),
            "Loser should be Blocked, got {:?}",
            status
        );

        // Should have emitted Blocked status
        let blocked_action = actions.iter().find(|a| {
            matches!(a, Action::EmitTransactionStatus { tx_hash, status: TransactionStatus::Blocked { .. }, .. } if *tx_hash == loser_hash)
        });
        assert!(
            blocked_action.is_some(),
            "Should have emitted Blocked status"
        );

        // Should NOT have pending deferral (it was processed immediately)
        assert!(
            !mempool.pending_deferrals.contains_key(&loser_hash),
            "Should not have pending deferral when tx was in same block"
        );
    }

    #[test]
    fn test_sync_multiple_blocks_with_dependencies() {
        // Scenario: Multi-block sync where:
        // - Block N: TX_A committed
        // - Block N+1: TX_A deferred (blocked by TX_B)
        // - Block N+2: TX_B's certificate commits, retry created for TX_A

        let mut mempool = MempoolState::new(make_test_topology());

        let tx_a = test_transaction(1);
        let tx_a_hash = tx_a.hash();
        let tx_b = test_transaction(2);
        let tx_b_hash = tx_b.hash();

        // Process blocks in order
        // Block N: TX_A committed
        let block_n = make_test_block(5, vec![tx_a.clone(), tx_b.clone()], vec![], vec![], vec![]);
        mempool.on_block_committed_full(&block_n);

        assert!(mempool.pool.contains_key(&tx_a_hash));
        assert!(mempool.pool.contains_key(&tx_b_hash));

        // Block N+1: TX_A deferred
        let deferral = TransactionDefer {
            tx_hash: tx_a_hash,
            reason: DeferReason::LivelockCycle {
                winner_tx_hash: tx_b_hash,
            },
            block_height: BlockHeight(6),
        };
        let block_n1 = make_test_block(6, vec![], vec![deferral], vec![], vec![]);
        mempool.on_block_committed_full(&block_n1);

        // TX_A should be Blocked
        assert!(matches!(
            mempool.status(&tx_a_hash),
            Some(TransactionStatus::Blocked { by }) if by == tx_b_hash
        ));

        // Block N+2: TX_B's certificate commits
        let tx_b_cert = make_test_certificate(tx_b_hash);
        let block_n2 = make_test_block(7, vec![], vec![], vec![tx_b_cert], vec![]);
        let actions = mempool.on_block_committed_full(&block_n2);

        // Retry should be created for TX_A
        let retried_action = actions.iter().find(|a| {
            matches!(a, Action::EmitTransactionStatus { tx_hash, status: TransactionStatus::Retried { .. }, .. } if *tx_hash == tx_a_hash)
        });
        assert!(
            retried_action.is_some(),
            "Should have created retry for TX_A when TX_B's cert committed"
        );

        // TX_A should be evicted from pool
        assert!(
            mempool.status(&tx_a_hash).is_none(),
            "TX_A should be evicted after retry created"
        );
    }

    // ═══════════════════════════════════════════════════════════════════════════
    // Tombstone Tests
    // ═══════════════════════════════════════════════════════════════════════════

    #[test]
    fn test_completed_transaction_is_tombstoned() {
        let mut mempool = MempoolState::new(make_test_topology());

        let tx = test_transaction(1);
        let tx_hash = tx.hash();

        // Submit and commit the transaction
        mempool.on_submit_transaction(tx.clone());
        let commit_block = make_test_block(1, vec![tx], vec![], vec![], vec![]);
        mempool.on_block_committed_full(&commit_block);

        // Commit the certificate
        let cert = make_test_certificate(tx_hash);
        let cert_block = make_test_block(2, vec![], vec![], vec![cert], vec![]);
        mempool.on_block_committed_full(&cert_block);

        // Transaction should be tombstoned
        assert!(mempool.is_tombstoned(&tx_hash));
        assert!(mempool.status(&tx_hash).is_none());
    }

    #[test]
    fn test_tombstoned_transaction_rejected_on_gossip() {
        let mut mempool = MempoolState::new(make_test_topology());

        let tx = test_transaction(1);
        let tx_hash = tx.hash();

        // Submit and complete the transaction
        mempool.on_submit_transaction(tx.clone());
        let commit_block = make_test_block(1, vec![tx.clone()], vec![], vec![], vec![]);
        mempool.on_block_committed_full(&commit_block);

        let cert = make_test_certificate(tx_hash);
        let cert_block = make_test_block(2, vec![], vec![], vec![cert], vec![]);
        mempool.on_block_committed_full(&cert_block);

        // Verify it's tombstoned
        assert!(mempool.is_tombstoned(&tx_hash));

        // Try to re-add via gossip - should be rejected
        let actions = mempool.on_transaction_gossip(tx.clone());
        assert!(actions.is_empty(), "Tombstoned tx should be rejected");

        // Should still not be in pool
        assert!(mempool.status(&tx_hash).is_none());
    }

    #[test]
    fn test_tombstoned_transaction_rejected_on_submit() {
        let mut mempool = MempoolState::new(make_test_topology());

        let tx = test_transaction(1);
        let tx_hash = tx.hash();

        // Submit and complete the transaction
        mempool.on_submit_transaction(tx.clone());
        let commit_block = make_test_block(1, vec![tx.clone()], vec![], vec![], vec![]);
        mempool.on_block_committed_full(&commit_block);

        let cert = make_test_certificate(tx_hash);
        let cert_block = make_test_block(2, vec![], vec![], vec![cert], vec![]);
        mempool.on_block_committed_full(&cert_block);

        // Try to re-submit - should be rejected (no status emitted)
        let actions = mempool.on_submit_transaction(tx.clone());
        assert!(actions.is_empty(), "Tombstoned tx should be rejected");

        // Should still not be in pool
        assert!(mempool.status(&tx_hash).is_none());
    }

    #[test]
    fn test_aborted_transaction_is_tombstoned() {
        let mut mempool = MempoolState::new(make_test_topology());

        let tx = test_transaction(1);
        let tx_hash = tx.hash();

        // Submit and commit the transaction
        mempool.on_submit_transaction(tx.clone());

        // Abort the transaction
        let abort = TransactionAbort {
            tx_hash,
            reason: AbortReason::ExecutionTimeout {
                committed_at: BlockHeight(1),
            },
            block_height: BlockHeight(2),
        };
        let abort_block = make_test_block(2, vec![], vec![], vec![], vec![abort]);
        mempool.on_block_committed_full(&abort_block);

        // Transaction should be tombstoned
        assert!(mempool.is_tombstoned(&tx_hash));

        // Try to re-add via gossip - should be rejected
        let actions = mempool.on_transaction_gossip(tx);
        assert!(actions.is_empty(), "Aborted tx should be rejected");
    }

    #[test]
    fn test_retried_transaction_is_tombstoned() {
        let mut mempool = MempoolState::new(make_test_topology());

        let loser = test_transaction(1);
        let loser_hash = loser.hash();
        let winner = test_transaction(2);
        let winner_hash = winner.hash();

        // Submit both transactions
        mempool.on_submit_transaction(loser.clone());
        mempool.on_submit_transaction(winner.clone());

        // Commit both
        let commit_block = make_test_block(
            1,
            vec![loser.clone(), winner.clone()],
            vec![],
            vec![],
            vec![],
        );
        mempool.on_block_committed_full(&commit_block);

        // Defer the loser
        let deferral = TransactionDefer {
            tx_hash: loser_hash,
            reason: DeferReason::LivelockCycle {
                winner_tx_hash: winner_hash,
            },
            block_height: BlockHeight(2),
        };
        let defer_block = make_test_block(2, vec![], vec![deferral], vec![], vec![]);
        mempool.on_block_committed_full(&defer_block);

        // Complete the winner - this creates a retry for the loser
        let cert = make_test_certificate(winner_hash);
        let cert_block = make_test_block(3, vec![], vec![], vec![cert], vec![]);
        mempool.on_block_committed_full(&cert_block);

        // Original loser should be tombstoned
        assert!(mempool.is_tombstoned(&loser_hash));

        // Try to re-add via gossip - should be rejected
        let actions = mempool.on_transaction_gossip(loser);
        assert!(actions.is_empty(), "Retried tx should be rejected");
    }

    #[test]
    fn test_tombstone_cleanup() {
        let mut mempool = MempoolState::new(make_test_topology());

        // Create and complete several transactions at different heights
        for i in 1..=5 {
            let tx = test_transaction(i);
            let tx_hash = tx.hash();

            mempool.on_submit_transaction(tx.clone());
            let commit_block = make_test_block(i as u64, vec![tx], vec![], vec![], vec![]);
            mempool.on_block_committed_full(&commit_block);

            let cert = make_test_certificate(tx_hash);
            let cert_block = make_test_block(i as u64 + 100, vec![], vec![], vec![cert], vec![]);
            mempool.on_block_committed_full(&cert_block);
        }

        // Should have 5 tombstones
        assert_eq!(mempool.tombstone_count(), 5);

        // Cleanup with short retention - should remove some
        let cleaned = mempool.cleanup_old_tombstones(BlockHeight(110), 5);
        assert!(cleaned > 0, "Should have cleaned up some tombstones");

        // Cleanup with long retention - should remove all remaining
        let _cleaned = mempool.cleanup_old_tombstones(BlockHeight(200), 5);
        assert_eq!(mempool.tombstone_count(), 0);
    }

    #[test]
    fn test_tombstone_prevents_resurrection_during_sync() {
        // Scenario: During sync, we receive blocks in rapid succession.
        // A transaction completes in block N, but gossip from block N-1
        // arrives afterwards trying to re-add it.

        let mut mempool = MempoolState::new(make_test_topology());

        let tx = test_transaction(1);
        let tx_hash = tx.hash();

        // Commit the transaction
        mempool.on_submit_transaction(tx.clone());
        let commit_block = make_test_block(10, vec![tx.clone()], vec![], vec![], vec![]);
        mempool.on_block_committed_full(&commit_block);

        // Complete the transaction
        let cert = make_test_certificate(tx_hash);
        let cert_block = make_test_block(11, vec![], vec![], vec![cert], vec![]);
        mempool.on_block_committed_full(&cert_block);

        // Pool should be empty
        assert_eq!(mempool.len(), 0);

        // Simulate late-arriving gossip
        let _actions = mempool.on_transaction_gossip(tx);

        // Pool should still be empty - tombstone prevented resurrection
        assert_eq!(mempool.len(), 0);
        assert!(mempool.is_tombstoned(&tx_hash));
    }

    #[test]
    fn test_mark_completed_creates_tombstone() {
        let mut mempool = MempoolState::new(make_test_topology());

        let tx = test_transaction(1);
        let tx_hash = tx.hash();

        // Submit the transaction
        mempool.on_submit_transaction(tx.clone());

        // Mark as completed directly
        mempool.mark_completed(&tx_hash, TransactionDecision::Accept);

        // Should be tombstoned
        assert!(mempool.is_tombstoned(&tx_hash));
        assert!(mempool.status(&tx_hash).is_none());

        // Should reject gossip
        let actions = mempool.on_transaction_gossip(tx);
        assert!(actions.is_empty());
    }

    // ═══════════════════════════════════════════════════════════════════════════
    // RPC Backpressure Tests
    // ═══════════════════════════════════════════════════════════════════════════

    #[test]
    fn test_is_accepting_rpc_transactions_default() {
        let mempool = MempoolState::new(make_test_topology());
        // Default config has DEFAULT_RPC_MEMPOOL_LIMIT, empty pool should accept
        assert!(mempool.is_accepting_rpc_transactions());
        assert_eq!(mempool.max_rpc_pool_size(), Some(DEFAULT_RPC_MEMPOOL_LIMIT));
    }

    #[test]
    fn test_is_accepting_rpc_transactions_unlimited() {
        let config = MempoolConfig::unlimited();
        let mempool = MempoolState::with_config(make_test_topology(), config);
        assert!(mempool.is_accepting_rpc_transactions());
        assert!(mempool.max_rpc_pool_size().is_none());
    }

    #[test]
    fn test_is_accepting_rpc_transactions_at_limit() {
        // Create config with very small limit
        let config = MempoolConfig::new().with_max_rpc_pool_size(Some(3));
        let mut mempool = MempoolState::with_config(make_test_topology(), config);

        // Add 2 transactions - should still accept
        mempool.on_submit_transaction(test_transaction(1));
        mempool.on_submit_transaction(test_transaction(2));
        assert!(mempool.is_accepting_rpc_transactions());
        assert_eq!(mempool.len(), 2);

        // Add 3rd transaction - now at limit, should NOT accept more
        mempool.on_submit_transaction(test_transaction(3));
        assert!(!mempool.is_accepting_rpc_transactions());
        assert_eq!(mempool.len(), 3);
    }

    #[test]
    fn test_gossip_accepted_even_when_full() {
        // Create config with very small limit
        let config = MempoolConfig::new().with_max_rpc_pool_size(Some(2));
        let mut mempool = MempoolState::with_config(make_test_topology(), config);

        // Fill the pool to the limit
        mempool.on_submit_transaction(test_transaction(1));
        mempool.on_submit_transaction(test_transaction(2));
        assert!(!mempool.is_accepting_rpc_transactions());

        // Gossip should STILL be accepted (we need txs for block validation)
        let tx3 = test_transaction(3);
        let tx3_hash = tx3.hash();
        let actions = mempool.on_transaction_gossip(tx3);

        // Gossip doesn't emit actions (no status emission for gossip)
        assert!(actions.is_empty());
        // But the transaction should be in the pool
        assert!(mempool.has_transaction(&tx3_hash));
        assert_eq!(mempool.len(), 3);
    }

    #[test]
    fn test_config_builder() {
        let config = MempoolConfig::new().with_max_rpc_pool_size(Some(50_000));
        assert_eq!(config.max_rpc_pool_size, Some(50_000));

        let config_none = MempoolConfig::new().with_max_rpc_pool_size(None);
        assert_eq!(config_none.max_rpc_pool_size, None);
    }
}
